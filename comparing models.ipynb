{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR: 0.744216 (0.189749)\n",
      "DT: 0.728808 (0.222525)\n",
      "RF: 0.758194 (0.184038)\n",
      "Ada: 0.727303 (0.221179)\n",
      "NB: 0.716308 (0.222414)\n",
      "MLP: 0.750711 (0.190771)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEVCAYAAAAb/KWvAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Compare Algorithms\n",
    "import pandas\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import model_selection\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import precision_score, f1_score, cohen_kappa_score, roc_auc_score, matthews_corrcoef\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "# load dataset\n",
    "\n",
    "names = [ 'Num.Comments',\n",
    " 'Avg Vote',\n",
    " 'Total Votes',\n",
    " 'success']\n",
    "#dataframe = pandas.read_excel(\"jockey3.xlsx\", names=names)\n",
    "\n",
    "df=df.dropna()\n",
    "X = df.drop('success', axis=1)  \n",
    "Y = df['success'] \n",
    "# prepare configuration for cross validation test harness\n",
    "seed = 7\n",
    "# prepare models\n",
    "models = []\n",
    "\n",
    "models.append(('LR', LogisticRegression()))\n",
    "models.append(('DT', DecisionTreeClassifier(max_depth=6)))\n",
    "models.append(('RF', RandomForestClassifier(max_depth=6)))\n",
    "models.append(('Ada', AdaBoostClassifier()))\n",
    "models.append(('NB', GaussianNB()))\n",
    "models.append(('MLP', MLPClassifier(hidden_layer_sizes=(10,10,10),max_iter=5000)))\n",
    "# evaluate each model in turn\n",
    "results = []\n",
    "names = []\n",
    "scoring = 'accuracy'\n",
    "for name, model in models:\n",
    "    kfold = model_selection.KFold(n_splits=10, random_state=seed)\n",
    "    cv_results = model_selection.cross_val_score(model, X, Y, cv=kfold, scoring=scoring)\n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
    "    print(msg)\n",
    "# boxplot algorithm comparison\n",
    "fig = plt.figure()\n",
    "fig.suptitle('Algorithm Comparison of Model Accuracy')\n",
    "ax = fig.add_subplot(111)\n",
    "plt.boxplot(results)\n",
    "ax.set_xticklabels(names)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.313433</td>\n",
       "      <td>0.452736</td>\n",
       "      <td>0.950249</td>\n",
       "      <td>0.850746</td>\n",
       "      <td>0.855</td>\n",
       "      <td>0.805</td>\n",
       "      <td>0.850</td>\n",
       "      <td>0.760</td>\n",
       "      <td>0.775</td>\n",
       "      <td>0.830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.313433</td>\n",
       "      <td>0.447761</td>\n",
       "      <td>0.950249</td>\n",
       "      <td>0.850746</td>\n",
       "      <td>0.855</td>\n",
       "      <td>0.805</td>\n",
       "      <td>0.850</td>\n",
       "      <td>0.750</td>\n",
       "      <td>0.780</td>\n",
       "      <td>0.835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.313433</td>\n",
       "      <td>0.557214</td>\n",
       "      <td>0.885572</td>\n",
       "      <td>0.766169</td>\n",
       "      <td>0.835</td>\n",
       "      <td>0.785</td>\n",
       "      <td>0.795</td>\n",
       "      <td>0.780</td>\n",
       "      <td>0.785</td>\n",
       "      <td>0.790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.313433</td>\n",
       "      <td>0.462687</td>\n",
       "      <td>0.840796</td>\n",
       "      <td>0.810945</td>\n",
       "      <td>0.805</td>\n",
       "      <td>0.635</td>\n",
       "      <td>0.865</td>\n",
       "      <td>0.705</td>\n",
       "      <td>0.740</td>\n",
       "      <td>0.810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.313433</td>\n",
       "      <td>0.268657</td>\n",
       "      <td>0.950249</td>\n",
       "      <td>0.850746</td>\n",
       "      <td>0.855</td>\n",
       "      <td>0.800</td>\n",
       "      <td>0.860</td>\n",
       "      <td>0.685</td>\n",
       "      <td>0.775</td>\n",
       "      <td>0.805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.313433</td>\n",
       "      <td>0.467662</td>\n",
       "      <td>0.950249</td>\n",
       "      <td>0.850746</td>\n",
       "      <td>0.865</td>\n",
       "      <td>0.800</td>\n",
       "      <td>0.860</td>\n",
       "      <td>0.780</td>\n",
       "      <td>0.825</td>\n",
       "      <td>0.835</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3      4      5      6      7      8  \\\n",
       "0  0.313433  0.452736  0.950249  0.850746  0.855  0.805  0.850  0.760  0.775   \n",
       "1  0.313433  0.447761  0.950249  0.850746  0.855  0.805  0.850  0.750  0.780   \n",
       "2  0.313433  0.557214  0.885572  0.766169  0.835  0.785  0.795  0.780  0.785   \n",
       "3  0.313433  0.462687  0.840796  0.810945  0.805  0.635  0.865  0.705  0.740   \n",
       "4  0.313433  0.268657  0.950249  0.850746  0.855  0.800  0.860  0.685  0.775   \n",
       "5  0.313433  0.467662  0.950249  0.850746  0.865  0.800  0.860  0.780  0.825   \n",
       "\n",
       "       9  \n",
       "0  0.830  \n",
       "1  0.835  \n",
       "2  0.790  \n",
       "3  0.810  \n",
       "4  0.805  \n",
       "5  0.835  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = pandas.DataFrame(results)\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataframe = pandas.read_excel(\"jockey3.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask=[\n",
    " 'Num.Comments',\n",
    " 'Avg Vote',\n",
    " 'Total Votes',\n",
    " 'success'\n",
    " ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df=dataframe[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 'accuracy'): LR: 0.744216 (0.189749)\n",
      "(0, 'accuracy'): DT: 0.728308 (0.222302)\n",
      "(0, 'accuracy'): RF: 0.752709 (0.188030)\n",
      "(0, 'accuracy'): Ada: 0.746706 (0.186805)\n",
      "(0, 'accuracy'): NB: 0.716308 (0.222414)\n",
      "(0, 'accuracy'): MLP: 0.786042 (0.163989)\n",
      "(1, 'precision_score'): LR: 0.744216 (0.189749)\n",
      "(1, 'precision_score'): DT: 0.748211 (0.187729)\n",
      "(1, 'precision_score'): RF: 0.755711 (0.190332)\n",
      "(1, 'precision_score'): Ada: 0.746706 (0.186805)\n",
      "(1, 'precision_score'): NB: 0.716308 (0.222414)\n",
      "(1, 'precision_score'): MLP: 0.750224 (0.191194)\n",
      "(2, 'recall_score'): LR: 0.744216 (0.189749)\n",
      "(2, 'recall_score'): DT: 0.747711 (0.187517)\n",
      "(2, 'recall_score'): RF: 0.754209 (0.189041)\n",
      "(2, 'recall_score'): Ada: 0.746706 (0.186805)\n",
      "(2, 'recall_score'): NB: 0.716308 (0.222414)\n",
      "(2, 'recall_score'): MLP: 0.754214 (0.192366)\n",
      "(3, 'f1_score'): LR: 0.744216 (0.189749)\n",
      "(3, 'f1_score'): DT: 0.728808 (0.222525)\n",
      "(3, 'f1_score'): RF: 0.753729 (0.198066)\n",
      "(3, 'f1_score'): Ada: 0.743224 (0.192131)\n",
      "(3, 'f1_score'): NB: 0.716308 (0.222414)\n",
      "(3, 'f1_score'): MLP: 0.754716 (0.192804)\n",
      "(4, 'cohen_kappa_score'): LR: 0.744216 (0.189749)\n",
      "(4, 'cohen_kappa_score'): DT: 0.747711 (0.187517)\n",
      "(4, 'cohen_kappa_score'): RF: 0.753709 (0.189748)\n",
      "(4, 'cohen_kappa_score'): Ada: 0.727303 (0.221179)\n",
      "(4, 'cohen_kappa_score'): NB: 0.716308 (0.222414)\n",
      "(4, 'cohen_kappa_score'): MLP: 0.788050 (0.164706)\n",
      "(5, 'm.matthews_corrcoef'): LR: 0.744216 (0.189749)\n",
      "(5, 'm.matthews_corrcoef'): DT: 0.728308 (0.222302)\n",
      "(5, 'm.matthews_corrcoef'): RF: 0.785542 (0.164593)\n",
      "(5, 'm.matthews_corrcoef'): Ada: 0.746706 (0.186805)\n",
      "(5, 'm.matthews_corrcoef'): NB: 0.716308 (0.222414)\n",
      "(5, 'm.matthews_corrcoef'): MLP: 0.786555 (0.165091)\n"
     ]
    }
   ],
   "source": [
    "for score in enumerate(scores):\n",
    "    for name, model in models:\n",
    "        kfold = model_selection.KFold(n_splits=10, random_state=seed)\n",
    "        cv_results = model_selection.cross_val_score(model, X, Y, cv=kfold, scoring=scoring)\n",
    "        results.append(cv_results)\n",
    "        names.append(name)\n",
    "        msg = \"%s: %s: %f (%f)\" % (score, name, cv_results.mean(), cv_results.std())\n",
    "        print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scores=['accuracy','precision_score', 'recall_score', 'f1_score', 'cohen_kappa_score', 'm.matthews_corrcoef']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "r=pandas.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.313433</td>\n",
       "      <td>0.452736</td>\n",
       "      <td>0.950249</td>\n",
       "      <td>0.850746</td>\n",
       "      <td>0.855</td>\n",
       "      <td>0.805</td>\n",
       "      <td>0.850</td>\n",
       "      <td>0.760</td>\n",
       "      <td>0.775</td>\n",
       "      <td>0.830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.313433</td>\n",
       "      <td>0.273632</td>\n",
       "      <td>0.950249</td>\n",
       "      <td>0.845771</td>\n",
       "      <td>0.825</td>\n",
       "      <td>0.810</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0.775</td>\n",
       "      <td>0.805</td>\n",
       "      <td>0.810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.313433</td>\n",
       "      <td>0.422886</td>\n",
       "      <td>0.950249</td>\n",
       "      <td>0.850746</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0.805</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0.780</td>\n",
       "      <td>0.815</td>\n",
       "      <td>0.840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.313433</td>\n",
       "      <td>0.472637</td>\n",
       "      <td>0.950249</td>\n",
       "      <td>0.850746</td>\n",
       "      <td>0.855</td>\n",
       "      <td>0.805</td>\n",
       "      <td>0.850</td>\n",
       "      <td>0.765</td>\n",
       "      <td>0.775</td>\n",
       "      <td>0.830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.313433</td>\n",
       "      <td>0.268657</td>\n",
       "      <td>0.950249</td>\n",
       "      <td>0.850746</td>\n",
       "      <td>0.855</td>\n",
       "      <td>0.800</td>\n",
       "      <td>0.860</td>\n",
       "      <td>0.685</td>\n",
       "      <td>0.775</td>\n",
       "      <td>0.805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.313433</td>\n",
       "      <td>0.945274</td>\n",
       "      <td>0.945274</td>\n",
       "      <td>0.850746</td>\n",
       "      <td>0.855</td>\n",
       "      <td>0.795</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0.760</td>\n",
       "      <td>0.825</td>\n",
       "      <td>0.845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.313433</td>\n",
       "      <td>0.452736</td>\n",
       "      <td>0.950249</td>\n",
       "      <td>0.850746</td>\n",
       "      <td>0.855</td>\n",
       "      <td>0.805</td>\n",
       "      <td>0.850</td>\n",
       "      <td>0.760</td>\n",
       "      <td>0.775</td>\n",
       "      <td>0.830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.313433</td>\n",
       "      <td>0.273632</td>\n",
       "      <td>0.950249</td>\n",
       "      <td>0.845771</td>\n",
       "      <td>0.825</td>\n",
       "      <td>0.810</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0.775</td>\n",
       "      <td>0.805</td>\n",
       "      <td>0.810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.313433</td>\n",
       "      <td>0.756219</td>\n",
       "      <td>0.950249</td>\n",
       "      <td>0.850746</td>\n",
       "      <td>0.860</td>\n",
       "      <td>0.810</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0.785</td>\n",
       "      <td>0.805</td>\n",
       "      <td>0.815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.313433</td>\n",
       "      <td>0.472637</td>\n",
       "      <td>0.950249</td>\n",
       "      <td>0.850746</td>\n",
       "      <td>0.855</td>\n",
       "      <td>0.805</td>\n",
       "      <td>0.850</td>\n",
       "      <td>0.765</td>\n",
       "      <td>0.775</td>\n",
       "      <td>0.830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.313433</td>\n",
       "      <td>0.268657</td>\n",
       "      <td>0.950249</td>\n",
       "      <td>0.850746</td>\n",
       "      <td>0.855</td>\n",
       "      <td>0.800</td>\n",
       "      <td>0.860</td>\n",
       "      <td>0.685</td>\n",
       "      <td>0.775</td>\n",
       "      <td>0.805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.313433</td>\n",
       "      <td>0.800995</td>\n",
       "      <td>0.945274</td>\n",
       "      <td>0.850746</td>\n",
       "      <td>0.860</td>\n",
       "      <td>0.805</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0.770</td>\n",
       "      <td>0.805</td>\n",
       "      <td>0.830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.313433</td>\n",
       "      <td>0.452736</td>\n",
       "      <td>0.950249</td>\n",
       "      <td>0.850746</td>\n",
       "      <td>0.855</td>\n",
       "      <td>0.805</td>\n",
       "      <td>0.850</td>\n",
       "      <td>0.760</td>\n",
       "      <td>0.775</td>\n",
       "      <td>0.830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.313433</td>\n",
       "      <td>0.467662</td>\n",
       "      <td>0.950249</td>\n",
       "      <td>0.845771</td>\n",
       "      <td>0.825</td>\n",
       "      <td>0.810</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0.775</td>\n",
       "      <td>0.805</td>\n",
       "      <td>0.810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.313433</td>\n",
       "      <td>0.472637</td>\n",
       "      <td>0.950249</td>\n",
       "      <td>0.850746</td>\n",
       "      <td>0.880</td>\n",
       "      <td>0.805</td>\n",
       "      <td>0.865</td>\n",
       "      <td>0.785</td>\n",
       "      <td>0.810</td>\n",
       "      <td>0.835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.313433</td>\n",
       "      <td>0.472637</td>\n",
       "      <td>0.950249</td>\n",
       "      <td>0.850746</td>\n",
       "      <td>0.855</td>\n",
       "      <td>0.805</td>\n",
       "      <td>0.850</td>\n",
       "      <td>0.765</td>\n",
       "      <td>0.775</td>\n",
       "      <td>0.830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.313433</td>\n",
       "      <td>0.268657</td>\n",
       "      <td>0.950249</td>\n",
       "      <td>0.850746</td>\n",
       "      <td>0.855</td>\n",
       "      <td>0.800</td>\n",
       "      <td>0.860</td>\n",
       "      <td>0.685</td>\n",
       "      <td>0.775</td>\n",
       "      <td>0.805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.313433</td>\n",
       "      <td>0.457711</td>\n",
       "      <td>0.945274</td>\n",
       "      <td>0.850746</td>\n",
       "      <td>0.855</td>\n",
       "      <td>0.795</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0.775</td>\n",
       "      <td>0.825</td>\n",
       "      <td>0.840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.313433</td>\n",
       "      <td>0.452736</td>\n",
       "      <td>0.950249</td>\n",
       "      <td>0.850746</td>\n",
       "      <td>0.855</td>\n",
       "      <td>0.805</td>\n",
       "      <td>0.850</td>\n",
       "      <td>0.760</td>\n",
       "      <td>0.775</td>\n",
       "      <td>0.830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.313433</td>\n",
       "      <td>0.273632</td>\n",
       "      <td>0.950249</td>\n",
       "      <td>0.845771</td>\n",
       "      <td>0.825</td>\n",
       "      <td>0.810</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0.775</td>\n",
       "      <td>0.805</td>\n",
       "      <td>0.810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.313433</td>\n",
       "      <td>0.467662</td>\n",
       "      <td>0.950249</td>\n",
       "      <td>0.850746</td>\n",
       "      <td>0.880</td>\n",
       "      <td>0.805</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0.780</td>\n",
       "      <td>0.805</td>\n",
       "      <td>0.830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.313433</td>\n",
       "      <td>0.472637</td>\n",
       "      <td>0.950249</td>\n",
       "      <td>0.850746</td>\n",
       "      <td>0.855</td>\n",
       "      <td>0.805</td>\n",
       "      <td>0.850</td>\n",
       "      <td>0.765</td>\n",
       "      <td>0.775</td>\n",
       "      <td>0.830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.313433</td>\n",
       "      <td>0.268657</td>\n",
       "      <td>0.950249</td>\n",
       "      <td>0.850746</td>\n",
       "      <td>0.855</td>\n",
       "      <td>0.800</td>\n",
       "      <td>0.860</td>\n",
       "      <td>0.685</td>\n",
       "      <td>0.775</td>\n",
       "      <td>0.805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.313433</td>\n",
       "      <td>0.920398</td>\n",
       "      <td>0.940299</td>\n",
       "      <td>0.850746</td>\n",
       "      <td>0.880</td>\n",
       "      <td>0.805</td>\n",
       "      <td>0.850</td>\n",
       "      <td>0.775</td>\n",
       "      <td>0.815</td>\n",
       "      <td>0.845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.313433</td>\n",
       "      <td>0.452736</td>\n",
       "      <td>0.950249</td>\n",
       "      <td>0.850746</td>\n",
       "      <td>0.855</td>\n",
       "      <td>0.805</td>\n",
       "      <td>0.850</td>\n",
       "      <td>0.760</td>\n",
       "      <td>0.775</td>\n",
       "      <td>0.830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.313433</td>\n",
       "      <td>0.467662</td>\n",
       "      <td>0.950249</td>\n",
       "      <td>0.845771</td>\n",
       "      <td>0.825</td>\n",
       "      <td>0.810</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0.775</td>\n",
       "      <td>0.805</td>\n",
       "      <td>0.810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.313433</td>\n",
       "      <td>0.800995</td>\n",
       "      <td>0.950249</td>\n",
       "      <td>0.850746</td>\n",
       "      <td>0.880</td>\n",
       "      <td>0.805</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0.780</td>\n",
       "      <td>0.815</td>\n",
       "      <td>0.835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.313433</td>\n",
       "      <td>0.437811</td>\n",
       "      <td>0.950249</td>\n",
       "      <td>0.850746</td>\n",
       "      <td>0.855</td>\n",
       "      <td>0.805</td>\n",
       "      <td>0.850</td>\n",
       "      <td>0.765</td>\n",
       "      <td>0.775</td>\n",
       "      <td>0.830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.313433</td>\n",
       "      <td>0.268657</td>\n",
       "      <td>0.950249</td>\n",
       "      <td>0.850746</td>\n",
       "      <td>0.855</td>\n",
       "      <td>0.800</td>\n",
       "      <td>0.860</td>\n",
       "      <td>0.685</td>\n",
       "      <td>0.775</td>\n",
       "      <td>0.805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.313433</td>\n",
       "      <td>0.457711</td>\n",
       "      <td>0.950249</td>\n",
       "      <td>0.850746</td>\n",
       "      <td>0.855</td>\n",
       "      <td>0.805</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0.770</td>\n",
       "      <td>0.795</td>\n",
       "      <td>0.840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.313433</td>\n",
       "      <td>0.452736</td>\n",
       "      <td>0.950249</td>\n",
       "      <td>0.850746</td>\n",
       "      <td>0.855</td>\n",
       "      <td>0.805</td>\n",
       "      <td>0.850</td>\n",
       "      <td>0.760</td>\n",
       "      <td>0.775</td>\n",
       "      <td>0.830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.313433</td>\n",
       "      <td>0.467662</td>\n",
       "      <td>0.950249</td>\n",
       "      <td>0.845771</td>\n",
       "      <td>0.830</td>\n",
       "      <td>0.810</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0.775</td>\n",
       "      <td>0.805</td>\n",
       "      <td>0.810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.313433</td>\n",
       "      <td>0.472637</td>\n",
       "      <td>0.950249</td>\n",
       "      <td>0.850746</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0.805</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0.785</td>\n",
       "      <td>0.800</td>\n",
       "      <td>0.835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.313433</td>\n",
       "      <td>0.472637</td>\n",
       "      <td>0.950249</td>\n",
       "      <td>0.850746</td>\n",
       "      <td>0.855</td>\n",
       "      <td>0.805</td>\n",
       "      <td>0.850</td>\n",
       "      <td>0.765</td>\n",
       "      <td>0.775</td>\n",
       "      <td>0.830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.313433</td>\n",
       "      <td>0.268657</td>\n",
       "      <td>0.950249</td>\n",
       "      <td>0.850746</td>\n",
       "      <td>0.855</td>\n",
       "      <td>0.800</td>\n",
       "      <td>0.860</td>\n",
       "      <td>0.685</td>\n",
       "      <td>0.775</td>\n",
       "      <td>0.805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.313433</td>\n",
       "      <td>0.467662</td>\n",
       "      <td>0.950249</td>\n",
       "      <td>0.850746</td>\n",
       "      <td>0.860</td>\n",
       "      <td>0.800</td>\n",
       "      <td>0.880</td>\n",
       "      <td>0.765</td>\n",
       "      <td>0.800</td>\n",
       "      <td>0.840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.313433</td>\n",
       "      <td>0.452736</td>\n",
       "      <td>0.950249</td>\n",
       "      <td>0.850746</td>\n",
       "      <td>0.855</td>\n",
       "      <td>0.805</td>\n",
       "      <td>0.850</td>\n",
       "      <td>0.760</td>\n",
       "      <td>0.775</td>\n",
       "      <td>0.830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.313433</td>\n",
       "      <td>0.273632</td>\n",
       "      <td>0.950249</td>\n",
       "      <td>0.845771</td>\n",
       "      <td>0.830</td>\n",
       "      <td>0.810</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0.775</td>\n",
       "      <td>0.805</td>\n",
       "      <td>0.810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.313433</td>\n",
       "      <td>0.417910</td>\n",
       "      <td>0.945274</td>\n",
       "      <td>0.850746</td>\n",
       "      <td>0.865</td>\n",
       "      <td>0.805</td>\n",
       "      <td>0.865</td>\n",
       "      <td>0.785</td>\n",
       "      <td>0.800</td>\n",
       "      <td>0.830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.313433</td>\n",
       "      <td>0.472637</td>\n",
       "      <td>0.950249</td>\n",
       "      <td>0.850746</td>\n",
       "      <td>0.855</td>\n",
       "      <td>0.805</td>\n",
       "      <td>0.850</td>\n",
       "      <td>0.765</td>\n",
       "      <td>0.775</td>\n",
       "      <td>0.830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.313433</td>\n",
       "      <td>0.268657</td>\n",
       "      <td>0.950249</td>\n",
       "      <td>0.850746</td>\n",
       "      <td>0.855</td>\n",
       "      <td>0.800</td>\n",
       "      <td>0.860</td>\n",
       "      <td>0.685</td>\n",
       "      <td>0.775</td>\n",
       "      <td>0.805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.313433</td>\n",
       "      <td>0.462687</td>\n",
       "      <td>0.950249</td>\n",
       "      <td>0.850746</td>\n",
       "      <td>0.855</td>\n",
       "      <td>0.810</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0.750</td>\n",
       "      <td>0.830</td>\n",
       "      <td>0.845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.313433</td>\n",
       "      <td>0.452736</td>\n",
       "      <td>0.950249</td>\n",
       "      <td>0.850746</td>\n",
       "      <td>0.855</td>\n",
       "      <td>0.805</td>\n",
       "      <td>0.850</td>\n",
       "      <td>0.760</td>\n",
       "      <td>0.775</td>\n",
       "      <td>0.830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.313433</td>\n",
       "      <td>0.467662</td>\n",
       "      <td>0.950249</td>\n",
       "      <td>0.845771</td>\n",
       "      <td>0.830</td>\n",
       "      <td>0.810</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0.775</td>\n",
       "      <td>0.805</td>\n",
       "      <td>0.810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.313433</td>\n",
       "      <td>0.467662</td>\n",
       "      <td>0.950249</td>\n",
       "      <td>0.850746</td>\n",
       "      <td>0.860</td>\n",
       "      <td>0.805</td>\n",
       "      <td>0.850</td>\n",
       "      <td>0.785</td>\n",
       "      <td>0.815</td>\n",
       "      <td>0.840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.313433</td>\n",
       "      <td>0.278607</td>\n",
       "      <td>0.950249</td>\n",
       "      <td>0.850746</td>\n",
       "      <td>0.855</td>\n",
       "      <td>0.805</td>\n",
       "      <td>0.850</td>\n",
       "      <td>0.765</td>\n",
       "      <td>0.775</td>\n",
       "      <td>0.830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.313433</td>\n",
       "      <td>0.268657</td>\n",
       "      <td>0.950249</td>\n",
       "      <td>0.850746</td>\n",
       "      <td>0.855</td>\n",
       "      <td>0.800</td>\n",
       "      <td>0.860</td>\n",
       "      <td>0.685</td>\n",
       "      <td>0.775</td>\n",
       "      <td>0.805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.313433</td>\n",
       "      <td>0.567164</td>\n",
       "      <td>0.945274</td>\n",
       "      <td>0.850746</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0.805</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0.775</td>\n",
       "      <td>0.825</td>\n",
       "      <td>0.845</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           0         1         2         3      4      5      6      7      8  \\\n",
       "0   0.313433  0.452736  0.950249  0.850746  0.855  0.805  0.850  0.760  0.775   \n",
       "1   0.313433  0.273632  0.950249  0.845771  0.825  0.810  0.875  0.775  0.805   \n",
       "2   0.313433  0.422886  0.950249  0.850746  0.875  0.805  0.875  0.780  0.815   \n",
       "3   0.313433  0.472637  0.950249  0.850746  0.855  0.805  0.850  0.765  0.775   \n",
       "4   0.313433  0.268657  0.950249  0.850746  0.855  0.800  0.860  0.685  0.775   \n",
       "5   0.313433  0.945274  0.945274  0.850746  0.855  0.795  0.875  0.760  0.825   \n",
       "6   0.313433  0.452736  0.950249  0.850746  0.855  0.805  0.850  0.760  0.775   \n",
       "7   0.313433  0.273632  0.950249  0.845771  0.825  0.810  0.875  0.775  0.805   \n",
       "8   0.313433  0.756219  0.950249  0.850746  0.860  0.810  0.875  0.785  0.805   \n",
       "9   0.313433  0.472637  0.950249  0.850746  0.855  0.805  0.850  0.765  0.775   \n",
       "10  0.313433  0.268657  0.950249  0.850746  0.855  0.800  0.860  0.685  0.775   \n",
       "11  0.313433  0.800995  0.945274  0.850746  0.860  0.805  0.875  0.770  0.805   \n",
       "12  0.313433  0.452736  0.950249  0.850746  0.855  0.805  0.850  0.760  0.775   \n",
       "13  0.313433  0.467662  0.950249  0.845771  0.825  0.810  0.875  0.775  0.805   \n",
       "14  0.313433  0.472637  0.950249  0.850746  0.880  0.805  0.865  0.785  0.810   \n",
       "15  0.313433  0.472637  0.950249  0.850746  0.855  0.805  0.850  0.765  0.775   \n",
       "16  0.313433  0.268657  0.950249  0.850746  0.855  0.800  0.860  0.685  0.775   \n",
       "17  0.313433  0.457711  0.945274  0.850746  0.855  0.795  0.875  0.775  0.825   \n",
       "18  0.313433  0.452736  0.950249  0.850746  0.855  0.805  0.850  0.760  0.775   \n",
       "19  0.313433  0.273632  0.950249  0.845771  0.825  0.810  0.875  0.775  0.805   \n",
       "20  0.313433  0.467662  0.950249  0.850746  0.880  0.805  0.875  0.780  0.805   \n",
       "21  0.313433  0.472637  0.950249  0.850746  0.855  0.805  0.850  0.765  0.775   \n",
       "22  0.313433  0.268657  0.950249  0.850746  0.855  0.800  0.860  0.685  0.775   \n",
       "23  0.313433  0.920398  0.940299  0.850746  0.880  0.805  0.850  0.775  0.815   \n",
       "24  0.313433  0.452736  0.950249  0.850746  0.855  0.805  0.850  0.760  0.775   \n",
       "25  0.313433  0.467662  0.950249  0.845771  0.825  0.810  0.875  0.775  0.805   \n",
       "26  0.313433  0.800995  0.950249  0.850746  0.880  0.805  0.875  0.780  0.815   \n",
       "27  0.313433  0.437811  0.950249  0.850746  0.855  0.805  0.850  0.765  0.775   \n",
       "28  0.313433  0.268657  0.950249  0.850746  0.855  0.800  0.860  0.685  0.775   \n",
       "29  0.313433  0.457711  0.950249  0.850746  0.855  0.805  0.875  0.770  0.795   \n",
       "30  0.313433  0.452736  0.950249  0.850746  0.855  0.805  0.850  0.760  0.775   \n",
       "31  0.313433  0.467662  0.950249  0.845771  0.830  0.810  0.875  0.775  0.805   \n",
       "32  0.313433  0.472637  0.950249  0.850746  0.875  0.805  0.875  0.785  0.800   \n",
       "33  0.313433  0.472637  0.950249  0.850746  0.855  0.805  0.850  0.765  0.775   \n",
       "34  0.313433  0.268657  0.950249  0.850746  0.855  0.800  0.860  0.685  0.775   \n",
       "35  0.313433  0.467662  0.950249  0.850746  0.860  0.800  0.880  0.765  0.800   \n",
       "36  0.313433  0.452736  0.950249  0.850746  0.855  0.805  0.850  0.760  0.775   \n",
       "37  0.313433  0.273632  0.950249  0.845771  0.830  0.810  0.875  0.775  0.805   \n",
       "38  0.313433  0.417910  0.945274  0.850746  0.865  0.805  0.865  0.785  0.800   \n",
       "39  0.313433  0.472637  0.950249  0.850746  0.855  0.805  0.850  0.765  0.775   \n",
       "40  0.313433  0.268657  0.950249  0.850746  0.855  0.800  0.860  0.685  0.775   \n",
       "41  0.313433  0.462687  0.950249  0.850746  0.855  0.810  0.875  0.750  0.830   \n",
       "42  0.313433  0.452736  0.950249  0.850746  0.855  0.805  0.850  0.760  0.775   \n",
       "43  0.313433  0.467662  0.950249  0.845771  0.830  0.810  0.875  0.775  0.805   \n",
       "44  0.313433  0.467662  0.950249  0.850746  0.860  0.805  0.850  0.785  0.815   \n",
       "45  0.313433  0.278607  0.950249  0.850746  0.855  0.805  0.850  0.765  0.775   \n",
       "46  0.313433  0.268657  0.950249  0.850746  0.855  0.800  0.860  0.685  0.775   \n",
       "47  0.313433  0.567164  0.945274  0.850746  0.875  0.805  0.875  0.775  0.825   \n",
       "\n",
       "        9  \n",
       "0   0.830  \n",
       "1   0.810  \n",
       "2   0.840  \n",
       "3   0.830  \n",
       "4   0.805  \n",
       "5   0.845  \n",
       "6   0.830  \n",
       "7   0.810  \n",
       "8   0.815  \n",
       "9   0.830  \n",
       "10  0.805  \n",
       "11  0.830  \n",
       "12  0.830  \n",
       "13  0.810  \n",
       "14  0.835  \n",
       "15  0.830  \n",
       "16  0.805  \n",
       "17  0.840  \n",
       "18  0.830  \n",
       "19  0.810  \n",
       "20  0.830  \n",
       "21  0.830  \n",
       "22  0.805  \n",
       "23  0.845  \n",
       "24  0.830  \n",
       "25  0.810  \n",
       "26  0.835  \n",
       "27  0.830  \n",
       "28  0.805  \n",
       "29  0.840  \n",
       "30  0.830  \n",
       "31  0.810  \n",
       "32  0.835  \n",
       "33  0.830  \n",
       "34  0.805  \n",
       "35  0.840  \n",
       "36  0.830  \n",
       "37  0.810  \n",
       "38  0.830  \n",
       "39  0.830  \n",
       "40  0.805  \n",
       "41  0.845  \n",
       "42  0.830  \n",
       "43  0.810  \n",
       "44  0.840  \n",
       "45  0.830  \n",
       "46  0.805  \n",
       "47  0.845  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0.31343284, 0.45273632, 0.95024876, 0.85074627, 0.855     ,\n",
       "        0.805     , 0.85      , 0.76      , 0.775     , 0.83      ]),\n",
       " array([0.31343284, 0.27363184, 0.95024876, 0.84577114, 0.825     ,\n",
       "        0.81      , 0.875     , 0.775     , 0.805     , 0.81      ]),\n",
       " array([0.31343284, 0.42288557, 0.95024876, 0.85074627, 0.875     ,\n",
       "        0.805     , 0.875     , 0.78      , 0.815     , 0.84      ]),\n",
       " array([0.31343284, 0.47263682, 0.95024876, 0.85074627, 0.855     ,\n",
       "        0.805     , 0.85      , 0.765     , 0.775     , 0.83      ]),\n",
       " array([0.31343284, 0.26865672, 0.95024876, 0.85074627, 0.855     ,\n",
       "        0.8       , 0.86      , 0.685     , 0.775     , 0.805     ]),\n",
       " array([0.31343284, 0.94527363, 0.94527363, 0.85074627, 0.855     ,\n",
       "        0.795     , 0.875     , 0.76      , 0.825     , 0.845     ]),\n",
       " array([0.31343284, 0.45273632, 0.95024876, 0.85074627, 0.855     ,\n",
       "        0.805     , 0.85      , 0.76      , 0.775     , 0.83      ]),\n",
       " array([0.31343284, 0.27363184, 0.95024876, 0.84577114, 0.825     ,\n",
       "        0.81      , 0.875     , 0.775     , 0.805     , 0.81      ]),\n",
       " array([0.31343284, 0.75621891, 0.95024876, 0.85074627, 0.86      ,\n",
       "        0.81      , 0.875     , 0.785     , 0.805     , 0.815     ]),\n",
       " array([0.31343284, 0.47263682, 0.95024876, 0.85074627, 0.855     ,\n",
       "        0.805     , 0.85      , 0.765     , 0.775     , 0.83      ]),\n",
       " array([0.31343284, 0.26865672, 0.95024876, 0.85074627, 0.855     ,\n",
       "        0.8       , 0.86      , 0.685     , 0.775     , 0.805     ]),\n",
       " array([0.31343284, 0.80099502, 0.94527363, 0.85074627, 0.86      ,\n",
       "        0.805     , 0.875     , 0.77      , 0.805     , 0.83      ]),\n",
       " array([0.31343284, 0.45273632, 0.95024876, 0.85074627, 0.855     ,\n",
       "        0.805     , 0.85      , 0.76      , 0.775     , 0.83      ]),\n",
       " array([0.31343284, 0.46766169, 0.95024876, 0.84577114, 0.825     ,\n",
       "        0.81      , 0.875     , 0.775     , 0.805     , 0.81      ]),\n",
       " array([0.31343284, 0.47263682, 0.95024876, 0.85074627, 0.88      ,\n",
       "        0.805     , 0.865     , 0.785     , 0.81      , 0.835     ]),\n",
       " array([0.31343284, 0.47263682, 0.95024876, 0.85074627, 0.855     ,\n",
       "        0.805     , 0.85      , 0.765     , 0.775     , 0.83      ]),\n",
       " array([0.31343284, 0.26865672, 0.95024876, 0.85074627, 0.855     ,\n",
       "        0.8       , 0.86      , 0.685     , 0.775     , 0.805     ]),\n",
       " array([0.31343284, 0.45771144, 0.94527363, 0.85074627, 0.855     ,\n",
       "        0.795     , 0.875     , 0.775     , 0.825     , 0.84      ]),\n",
       " array([0.31343284, 0.45273632, 0.95024876, 0.85074627, 0.855     ,\n",
       "        0.805     , 0.85      , 0.76      , 0.775     , 0.83      ]),\n",
       " array([0.31343284, 0.27363184, 0.95024876, 0.84577114, 0.825     ,\n",
       "        0.81      , 0.875     , 0.775     , 0.805     , 0.81      ]),\n",
       " array([0.31343284, 0.46766169, 0.95024876, 0.85074627, 0.88      ,\n",
       "        0.805     , 0.875     , 0.78      , 0.805     , 0.83      ]),\n",
       " array([0.31343284, 0.47263682, 0.95024876, 0.85074627, 0.855     ,\n",
       "        0.805     , 0.85      , 0.765     , 0.775     , 0.83      ]),\n",
       " array([0.31343284, 0.26865672, 0.95024876, 0.85074627, 0.855     ,\n",
       "        0.8       , 0.86      , 0.685     , 0.775     , 0.805     ]),\n",
       " array([0.31343284, 0.92039801, 0.94029851, 0.85074627, 0.88      ,\n",
       "        0.805     , 0.85      , 0.775     , 0.815     , 0.845     ]),\n",
       " array([0.31343284, 0.45273632, 0.95024876, 0.85074627, 0.855     ,\n",
       "        0.805     , 0.85      , 0.76      , 0.775     , 0.83      ]),\n",
       " array([0.31343284, 0.46766169, 0.95024876, 0.84577114, 0.825     ,\n",
       "        0.81      , 0.875     , 0.775     , 0.805     , 0.81      ]),\n",
       " array([0.31343284, 0.80099502, 0.95024876, 0.85074627, 0.88      ,\n",
       "        0.805     , 0.875     , 0.78      , 0.815     , 0.835     ]),\n",
       " array([0.31343284, 0.43781095, 0.95024876, 0.85074627, 0.855     ,\n",
       "        0.805     , 0.85      , 0.765     , 0.775     , 0.83      ]),\n",
       " array([0.31343284, 0.26865672, 0.95024876, 0.85074627, 0.855     ,\n",
       "        0.8       , 0.86      , 0.685     , 0.775     , 0.805     ]),\n",
       " array([0.31343284, 0.45771144, 0.95024876, 0.85074627, 0.855     ,\n",
       "        0.805     , 0.875     , 0.77      , 0.795     , 0.84      ]),\n",
       " array([0.31343284, 0.45273632, 0.95024876, 0.85074627, 0.855     ,\n",
       "        0.805     , 0.85      , 0.76      , 0.775     , 0.83      ]),\n",
       " array([0.31343284, 0.46766169, 0.95024876, 0.84577114, 0.83      ,\n",
       "        0.81      , 0.875     , 0.775     , 0.805     , 0.81      ]),\n",
       " array([0.31343284, 0.47263682, 0.95024876, 0.85074627, 0.875     ,\n",
       "        0.805     , 0.875     , 0.785     , 0.8       , 0.835     ]),\n",
       " array([0.31343284, 0.47263682, 0.95024876, 0.85074627, 0.855     ,\n",
       "        0.805     , 0.85      , 0.765     , 0.775     , 0.83      ]),\n",
       " array([0.31343284, 0.26865672, 0.95024876, 0.85074627, 0.855     ,\n",
       "        0.8       , 0.86      , 0.685     , 0.775     , 0.805     ]),\n",
       " array([0.31343284, 0.46766169, 0.95024876, 0.85074627, 0.86      ,\n",
       "        0.8       , 0.88      , 0.765     , 0.8       , 0.84      ]),\n",
       " array([0.31343284, 0.45273632, 0.95024876, 0.85074627, 0.855     ,\n",
       "        0.805     , 0.85      , 0.76      , 0.775     , 0.83      ]),\n",
       " array([0.31343284, 0.27363184, 0.95024876, 0.84577114, 0.83      ,\n",
       "        0.81      , 0.875     , 0.775     , 0.805     , 0.81      ]),\n",
       " array([0.31343284, 0.41791045, 0.94527363, 0.85074627, 0.865     ,\n",
       "        0.805     , 0.865     , 0.785     , 0.8       , 0.83      ]),\n",
       " array([0.31343284, 0.47263682, 0.95024876, 0.85074627, 0.855     ,\n",
       "        0.805     , 0.85      , 0.765     , 0.775     , 0.83      ]),\n",
       " array([0.31343284, 0.26865672, 0.95024876, 0.85074627, 0.855     ,\n",
       "        0.8       , 0.86      , 0.685     , 0.775     , 0.805     ]),\n",
       " array([0.31343284, 0.46268657, 0.95024876, 0.85074627, 0.855     ,\n",
       "        0.81      , 0.875     , 0.75      , 0.83      , 0.845     ]),\n",
       " array([0.31343284, 0.45273632, 0.95024876, 0.85074627, 0.855     ,\n",
       "        0.805     , 0.85      , 0.76      , 0.775     , 0.83      ]),\n",
       " array([0.31343284, 0.46766169, 0.95024876, 0.84577114, 0.83      ,\n",
       "        0.81      , 0.875     , 0.775     , 0.805     , 0.81      ]),\n",
       " array([0.31343284, 0.46766169, 0.95024876, 0.85074627, 0.86      ,\n",
       "        0.805     , 0.85      , 0.785     , 0.815     , 0.84      ]),\n",
       " array([0.31343284, 0.27860697, 0.95024876, 0.85074627, 0.855     ,\n",
       "        0.805     , 0.85      , 0.765     , 0.775     , 0.83      ]),\n",
       " array([0.31343284, 0.26865672, 0.95024876, 0.85074627, 0.855     ,\n",
       "        0.8       , 0.86      , 0.685     , 0.775     , 0.805     ]),\n",
       " array([0.31343284, 0.56716418, 0.94527363, 0.85074627, 0.875     ,\n",
       "        0.805     , 0.875     , 0.775     , 0.825     , 0.845     ])]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYYAAAEICAYAAABbOlNNAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Compare Algorithms\n",
    "import pandas\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import model_selection\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split  \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, cohen_kappa_score, roc_auc_score\n",
    "\n",
    "# load dataset\n",
    "\n",
    "names = [ 'Num.Comments',\n",
    " 'Avg Vote',\n",
    " 'Total Votes',\n",
    " 'success']\n",
    "#dataframe = pandas.read_excel(\"jockey3.xlsx\", names=names)\n",
    "\n",
    "df=df.dropna()\n",
    "X = df.drop('success', axis=1)  \n",
    "Y = df['success'] \n",
    "# prepare configuration for cross validation test harness\n",
    "seed = 7\n",
    "# prepare models\n",
    "models = []\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)  \n",
    "scaler = StandardScaler()\n",
    "# Fit only to the training data\n",
    "scaler.fit(X_train)\n",
    "StandardScaler(copy=True, with_mean=True, with_std=True)\n",
    "# Now apply the transformations to the data:\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "models.append(('LR', LogisticRegression()))\n",
    "models.append(('DT', DecisionTreeClassifier(max_depth=6)))\n",
    "models.append(('RF', RandomForestClassifier(max_depth=6)))\n",
    "models.append(('Ada', AdaBoostClassifier()))\n",
    "models.append(('NB', GaussianNB()))\n",
    "models.append(('MLP', MLPClassifier(hidden_layer_sizes=(10,10,10),max_iter=5000)))\n",
    "# evaluate each model in turn\n",
    "results = []\n",
    "names = []\n",
    "models = []\n",
    "scoring = cohen_kappa_score\n",
    "for name, model in models:\n",
    "    model.fit(X_train, y_train)\n",
    "    y_predict = model.predict(X_test)\n",
    "    \n",
    "    cv_results=scoring(y_test, y_predict)\n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "    msg = \"%s, %f\" % (name, cv_results)\n",
    "    print(msg)\n",
    "# boxplot algorithm comparison\n",
    "fig = plt.figure()\n",
    "fig.suptitle('Algorithm Comparison of Model Accuracy')\n",
    "ax = fig.add_subplot(111)\n",
    "plt.boxplot(results)\n",
    "ax.set_xticklabels(names)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR, 0.000000\n",
      "DT, 0.387097\n",
      "RF, 0.386555\n",
      "Ada, 0.290598\n",
      "NB, 0.320000\n",
      "MLP, 0.386555\n"
     ]
    }
   ],
   "source": [
    "\n",
    "scoring = f1_score\n",
    "for name, model in models:\n",
    "    model.fit(X_train, y_train)\n",
    "    y_predict = model.predict(X_test)\n",
    "    cv_results=scoring(y_test, y_predict)\n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "    msg = \"%s, %f\" % (name, cv_results)\n",
    "    print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'score'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-67-297688f7e7a7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mff\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecision_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mff\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'score'"
     ]
    }
   ],
   "source": [
    "\n",
    "model=LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "ff = model.decision_function(X_test)\n",
    "print(ff.score(X,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
